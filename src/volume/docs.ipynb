{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume Control using OpenCV and MediaPipe\n",
    "\n",
    "This code captures video from a camera, detects hand/finger movements, and changes the volume of the computer based on hand gestures. It uses OpenCV, Mediapipe, ctypes, and pycaw libraries to perform these operations. The user can increase the volume by bringing their index and middle fingers to the middle left of the screen and decrease it by bringing them to the middle right. The current volume level is displayed on the screen."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first thing we do is import necessary libraries like sys, cv2, mediapipe, math, ctypes, comtypes and numpy using `import` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from math import hypot\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a boolean variable called `DEBUG` which will be used later to print some debug information if the value is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is an argument in the command line pass it to the function set DEBUG to False\n",
    "DEBUG = argv[1] == \"--debug\" if len(argv) > 1 else False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program then creates a capture object using `cv2.VideoCapture(0)`. This checks if a camera is available and if it is, selects the first available camera. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # Checks for camera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the resolution and frame rate of the video feed using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Sets the width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Sets the height\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)  # Sets the fps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize the MediaPipe hands object and configure its options using `mp_hands = mp.solutions.hands` and `hands = mp_hands.Hands()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands  # detects hand/finger\n",
    "hands = mp_hands.Hands()  # complete the initialization configuration of hands\n",
    "mpDraw = mp.solutions.drawing_utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we access the speaker through the pycaw library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access speaker through the library pycaw\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "volbar = 400\n",
    "volper = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get some important configuration values for audio volume from our speaker interface using `volMin, volMax = volume.GetVolumeRange()[:2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume range\n",
    "volMin, volMax = volume.GetVolumeRange()[:2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the while loop, we read frames from the camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, frame = cap.read()  # If camera works capture an image\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We flip the image horizontally to make it easier for us to interact with the right hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip the image\n",
    "frame = cv2.flip(frame, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `hands.process(frame)` to process the frame and detect hand landmarks. If landmarks are available, we proceed to check for specific gestures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection of gesture information\n",
    "results = hands.process(frame)  # completes the image processing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the index and middle fingers are in the middle left of the screen using some calculations and comparisons. If they are, then we update our volume level accordingly by setting the master volume level of the audio interface using `volume.SetMasterVolumeLevel(vol, None)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmList = []  # empty list\n",
    "    if results.multi_hand_landmarks:  # list of all hands detected.\n",
    "        \n",
    "        # By accessing the list, we can get the information of each hand's corresponding flag bit\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        \n",
    "            # adding counter and returning it\n",
    "            for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                # Get finger joint points\n",
    "                h, w, _ = frame.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                # adding to the empty list 'lmList'\n",
    "                lmList.append([id, cx, cy])\n",
    "            \n",
    "            if DEBUG:\n",
    "                print(lmList)        \n",
    "                mpDraw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    if lmList != []:\n",
    "        \n",
    "        # if the index finger and middle finger are in the middle left of the screen\n",
    "        # increase the volume\n",
    "        # getting the value at a point as x, y\n",
    "        x1, y1 = lmList[4][1], lmList[4][2]  # thumb\n",
    "        x2, y2 = lmList[8][1], lmList[8][2]  # index finger\n",
    "\n",
    "        if x1 < frame.shape[1] // 2 and x2 < frame.shape[1] // 2 and y1 < frame.shape[0] // 2 and y2 < frame.shape[0] // 2:\n",
    "\n",
    "            # creating circle at the tips of thumb and index finger\n",
    "            # image #fingers #radius #rgb\n",
    "            cv2.circle(frame, (x1, y1), 13, (255, 0, 0), cv2.FILLED)\n",
    "            # image #fingers #radius #rgb\n",
    "            cv2.circle(frame, (x2, y2), 13, (255, 0, 0), cv2.FILLED)\n",
    "            # create a line b/w tips of index finger and thumb\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "\n",
    "            length = hypot(x2 - x1, y2 - y1)  \n",
    "            # distance b/w tips using hypotenuse\n",
    "            # from numpy we find our length by converting hand range in terms of volume range ie b/w -63.5 to 0\n",
    "            vol = np.interp(length, [30, 350], [volMin, volMax])\n",
    "            volbar = np.interp(length, [30, 350], [400, 150])\n",
    "            volper = np.interp(length, [30, 350], [0, 100])\n",
    "\n",
    "            volume.SetMasterVolumeLevel(vol, None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a rectangle and fill it up to the current volume level on the frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand range 30 - 350\n",
    "    # Volume range -63.5 - 0.0\n",
    "    # creating volume bar for volume level\n",
    "    # vid ,initial position ,ending position ,rgb ,thickness\n",
    "    cv2.rectangle(frame, (50, 150), (85, 400), (0, 0, 255), 4)\n",
    "    cv2.rectangle(frame, (50, int(volbar)), (85, 400),\n",
    "                (0, 0, 255), cv2.FILLED)\n",
    "    cv2.putText(frame, f\"{int(volper)}%\", (10, 40),\n",
    "                cv2.FONT_ITALIC, 1, (0, 255, 98), 3)\n",
    "    # tell the volume percentage ,location,font of text,length,rgb color,thickness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we display the video output on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Image', frame)  # Show the video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop runs until the capture device is stopped. User can stop the capture device by pressing 'q' key on their keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while the capture device is open run the loop, if q is pressed break\n",
    "if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13. After the while loop ends, release the camera resource and destroy all windows using `cap.release()` and `cv2.destroyAllWindows()`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()  # stop cam\n",
    "cv2.destroyAllWindows()  # close window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import argv\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from math import hypot\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import numpy as np\n",
    "\n",
    "DEBUG = argv[1] == '-d' or argv[1] == '--debug'\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Checks for camera\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)  # Sets the width\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)  # Sets the height\n",
    "cap.set(cv2.CAP_PROP_FPS, 30)  # Sets the fps\n",
    "\n",
    "mp_hands = mp.solutions.hands  # detects hand/finger\n",
    "hands = mp_hands.Hands()  # complete the initialization configuration of hands\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# To access speaker through the library pycaw\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "volbar = 400\n",
    "volper = 0\n",
    "\n",
    "# volume range\n",
    "volMin, volMax = volume.GetVolumeRange()[:2]\n",
    "\n",
    "# while the capture device is open run the loop\n",
    "while cap.isOpened():\n",
    "\n",
    "    success, frame = cap.read()  # If camera works capture an image\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    # flip the image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Collection of gesture information\n",
    "    results = hands.process(frame)  # completes the image processing.\n",
    "\n",
    "    lmList = []  # empty list\n",
    "    if results.multi_hand_landmarks:  # list of all hands detected.\n",
    "        \n",
    "        # By accessing the list, we can get the information of each hand's corresponding flag bit\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            \n",
    "            # Get the landmarks for the index and middle fingers\n",
    "            index_finger = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "            middle_finger = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
    "\n",
    "            # Check if the fingers are in the middle left of the screen\n",
    "            if index_finger.x < frame.shape[1] // 2 and middle_finger.x < frame.shape[1] // 2 and \\\n",
    "                    index_finger.y < frame.shape[0] // 2 and middle_finger.y < frame.shape[0] // 2:\n",
    "                print(\"Fingers location is middle left of the screen. Increase volume.\")\n",
    "            \n",
    "                # adding counter and returning it\n",
    "                for id, lm in enumerate(hand_landmarks.landmark):\n",
    "                    # Get finger joint points\n",
    "                    h, w, _ = frame.shape\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                    # adding to the empty list 'lmList'\n",
    "                    lmList.append([id, cx, cy])\n",
    "            \n",
    "            if DEBUG:\n",
    "                print(lmList)        \n",
    "                mpDraw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    if lmList != []:\n",
    "        \n",
    "        # if the index finger and middle finger are in the middle left of the screen\n",
    "        # increase the volume\n",
    "        # getting the value at a point as x, y\n",
    "        x1, y1 = lmList[4][1], lmList[4][2]  # thumb\n",
    "        x2, y2 = lmList[8][1], lmList[8][2]  # index finger\n",
    "\n",
    "        if x1 < frame.shape[1] // 2 and x2 < frame.shape[1] // 2 and y1 < frame.shape[0] // 2 and y2 < frame.shape[0] // 2:\n",
    "\n",
    "            # creating circle at the tips of thumb and index finger\n",
    "            # image #fingers #radius #rgb\n",
    "            cv2.circle(frame, (x1, y1), 13, (255, 0, 0), cv2.FILLED)\n",
    "            # image #fingers #radius #rgb\n",
    "            cv2.circle(frame, (x2, y2), 13, (255, 0, 0), cv2.FILLED)\n",
    "            # create a line b/w tips of index finger and thumb\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "\n",
    "            length = hypot(x2 - x1, y2 - y1)  \n",
    "            # distance b/w tips using hypotenuse\n",
    "            # from numpy we find our length by converting hand range in terms of volume range ie b/w -63.5 to 0\n",
    "            vol = np.interp(length, [30, 350], [volMin, volMax])\n",
    "            volbar = np.interp(length, [30, 350], [400, 150])\n",
    "            volper = np.interp(length, [30, 350], [0, 100])\n",
    "\n",
    "            volume.SetMasterVolumeLevel(vol, None)\n",
    "\n",
    "    # Hand range 30 - 350\n",
    "    # Volume range -63.5 - 0.0\n",
    "    # creating volume bar for volume level\n",
    "    # vid ,initial position ,ending position ,rgb ,thickness\n",
    "    cv2.rectangle(frame, (50, 150), (85, 400), (0, 0, 255), 4)\n",
    "    cv2.rectangle(frame, (50, int(volbar)), (85, 400),\n",
    "                (0, 0, 255), cv2.FILLED)\n",
    "    cv2.putText(frame, f\"{int(volper)}%\", (10, 40),\n",
    "                cv2.FONT_ITALIC, 1, (0, 255, 98), 3)\n",
    "    # tell the volume percentage ,location,font of text,length,rgb color,thickness\n",
    "\n",
    "    cv2.imshow('Image', frame)  # Show the video\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):  # By using spacebar delay will stop\n",
    "        break\n",
    "\n",
    "cap.release()  # stop cam\n",
    "cv2.destroyAllWindows()  # close window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
